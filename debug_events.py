#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Debug LangChain Events

This script helps debug what events are actually being generated by LangChain.
"""

import asyncio
import os
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import tool
from langchain_core.prompts import PromptTemplate


@tool
def get_weather(city: str) -> str:
    """Get the current weather for a city.
    
    Args:
        city: The name of the city to get weather for
        
    Returns:
        Weather information for the city
    """
    return f"The weather in {city} is sunny with 22Â°C temperature."


async def debug_langchain_events():
    """Debug what events LangChain actually generates"""
    # Temporarily disable LangSmith tracing
    os.environ["LANGCHAIN_TRACING_V2"] = "false"

    # Check for API key
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("Error: DEEPSEEK_API_KEY environment variable not set")
        return
    
    # Initialize the language model
    llm = ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=api_key,
        openai_api_base="https://api.deepseek.com",
        temperature=0.1,
        streaming=True
    )
    
    # Create tools list
    tools = [get_weather]
    
    # Create simple prompt
    prompt = PromptTemplate.from_template(
        """Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}
Thought:{agent_scratchpad}"""
    )
    
    # Create agent
    agent = create_react_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    
    # Test query
    test_query = "What's the weather in Beijing?"
    
    try:
        # Create agent stream using astream_events
        agent_stream = agent_executor.astream_events({"input": test_query}, version="v2")
        
        # Debug: print all events
        print("\nLangChain Events:")
        print("-" * 40)
        event_count = 0
        tool_events_found = False
        async for event in agent_stream:
            event_count += 1
            event_type = event.get("event", "unknown")
            event_name = event.get("name", "unknown")
            
            # Check for tool-related events or data
            is_tool_related = "tool" in event_type.lower() or "tool" in event_name.lower()
            event_data = event.get('data', {})
            
            # Check if data contains tool calls
            has_tool_calls = False
            if isinstance(event_data, dict):
                chunk = event_data.get('chunk')
                if chunk and hasattr(chunk, 'tool_calls') and chunk.tool_calls:
                    has_tool_calls = True
                    is_tool_related = True
            
            print(f"Event {event_count}: {event_type} - {event_name}")
            
            # Print detailed info for tool events or events with tool calls
            if is_tool_related or has_tool_calls:
                tool_events_found = True
                print(f"  ** TOOL EVENT DETECTED **")
                print(f"  Data: {event_data}")
                print(f"  Run ID: {event.get('run_id', 'N/A')}")
                if has_tool_calls:
                    chunk = event_data.get('chunk')
                    print(f"  Tool calls: {chunk.tool_calls}")
            
            # Also check chat_model_stream events for tool calls
            elif event_type == "on_chat_model_stream":
                chunk = event_data.get('chunk')
                if chunk and hasattr(chunk, 'tool_calls') and chunk.tool_calls:
                    tool_events_found = True
                    print(f"  ** TOOL CALLS IN CHAT STREAM **")
                    print(f"  Tool calls: {chunk.tool_calls}")
                elif chunk and hasattr(chunk, 'content') and chunk.content:
                    # Only print first few content events to avoid spam
                    if event_count <= 85 or event_count >= 95:
                        print(f"  Content: {repr(chunk.content)}")
            
            # Limit output to first 100 events
            if event_count >= 100:
                print("... (truncated)")
                break
        
        print("-" * 40)
        print(f"Total events processed: {event_count}")
        
    except Exception as e:
        print(f"Error during streaming: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(debug_langchain_events())